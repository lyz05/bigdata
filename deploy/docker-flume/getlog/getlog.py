# -*- coding: utf-8 -*-
"""专业综合实训.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FnxSnLNDX7JfIZC7NrD_4BeIPCT6VQ0S

# 数据采集
[本站访问日志现已公开发布](https://mirrors.tuna.tsinghua.edu.cn/news/release-logs/)
爬取网页并下载数据
"""

import requests
from bs4 import BeautifulSoup
import re
import subprocess
import threading
import datetime
import os
import multiprocessing


# 源日志地址
urls = ['https://mirrors.tuna.tsinghua.edu.cn/logs/neomirrors/','https://mirrors.tuna.tsinghua.edu.cn/logs/nanomirrors/']
# 目标HDFS日志地址
hdfs_dsts = ['/bigdata/neomirrors/','/bigdata/nanomirrors/']
# HDFS执行文件路径
hdfs_program = os.environ['HADOOP_HOME']+'/bin/hdfs'
# 正则表达式匹配用于过滤所需日志(过滤aosp开头的gz压缩文件)
regex = r'^aosp(.*?).gz$'
headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}
cmdlist = []
threads = []
# 获取CPU核心数作为线程数
threads_num = multiprocessing.cpu_count()

def execCmd(cmdlist):
	t = threading.currentThread()
	#线程NAME
	name = t.getName()
	try:
		i = 0
		for cmd in cmdlist:
			i += 1
			subprocess.check_call('wget -q --tries=0 -O /tmp/{1} {0[0]}{0[1]} && {0[3]} dfs -put -f /tmp/{1} {0[2]}{0[1]}'.format(cmd,name),shell=True)
			print("%s,处理%s完成(%d/%d)" % (name,"{0[0]}{0[1]}".format(cmd),i,len(cmdlist)))
	except Exception as e :       #异常处理
		print('%s,运行失败,失败原因\n%s' % (name,e))

if __name__ == '__main__':
	print("程序开始%s" % datetime.datetime.now())
	for hdfs_dst in hdfs_dsts:
		# 创建目录
		subprocess.call('{} dfs -mkdir -p {}'.format(hdfs_program,hdfs_dst),shell=True)

	for (url,hdfs_dst) in zip(urls,hdfs_dsts):
		data = requests.get(url,headers=headers).text
		soup = BeautifulSoup(data, 'html.parser')
		links = soup.find_all(name='a',text=re.compile(regex))  #提取所有a标签
		# 获取目标目录下文件列表
		ret = subprocess.check_output('{} dfs -ls {}'.format(hdfs_program,hdfs_dst),shell=True)
		ret = str(ret, encoding = "utf-8")
		#print(ret)
		for link in links:
			href = link['href']
			name = link.string
			# 生成需要下载的日志链接信息
			if name not in ret:
				cmdlist.append([url,href,hdfs_dst,hdfs_program])
	
	# 多线程下载
	length = len(cmdlist)  # 总长
	print('共有{}链接待下载'.format(length))
	step = int(length / threads_num) + 1  # 每份的长度
	for i in range(0, length, step):
		th = threading.Thread(target=execCmd, args=(cmdlist[i: i + step],))  #调用函数,引入线程参数
		th.start()          #开始执行
		threads.append(th)

	# 等待线程运行完毕
	for th in threads:
		th.join()       #循环 join()方法可以让主线程等待所有的线程都执行完毕
	print("程序结束%s" % datetime.datetime.now())